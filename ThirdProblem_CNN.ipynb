{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ยบ Problem - Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from keras import layers, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigthed_classes(y_train):\n",
    "\n",
    "    class_labels = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_labels))}\n",
    "\n",
    "    return class_weight_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Oversampling (SMOTE)\n",
    "\n",
    "- Generates synthetic examples rather than just duplicating existing minority class samples.\n",
    "- Helps improve the model's ability to generalize by providing diverse training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def oversample_with_smote(X_train, y_train,Y_train):\n",
    "    \n",
    "    num_classes = len(np.unique(Y_train))\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1) \n",
    "    X_train_os, y_train_os = smote.fit_resample(X_train_flat, y_train)\n",
    "    X_train_os = X_train_os.reshape(X_train_os.shape[0], 48, 48, 1)  \n",
    "    y_train_os = to_categorical(y_train_os, num_classes=num_classes) \n",
    "    return X_train_os, y_train_os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(X_train,y_train):\n",
    "    \n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_us, y_train_us = rus.fit_resample(X_train_flat, y_train)\n",
    "\n",
    "    X_train_us = X_train_us.reshape(X_train_us.shape[0], 48, 48, 1)\n",
    "\n",
    "    y_train_us= to_categorical(y_train_us, 2)\n",
    "\n",
    "    return X_train_us, y_train_us\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(cm):   \n",
    "    tn = cm[0, 0]  # True negatives\n",
    "    fp = cm[0, 1]  # False positives\n",
    "    fn = cm[1, 0]  # False negatives\n",
    "    tp = cm[1, 1]  # true positives\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f1_score1)\n",
    "\n",
    "    precision = tn/(tn+fn) if (tn+fn) > 0 else 0\n",
    "    recall = tn/(tn+fp) if tn+fp >0 else 0\n",
    "    f1_score2 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f1_score2)\n",
    "\n",
    "    f1_score  = (f1_score1 + f1_score2) / 2\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = recall  \n",
    "    balanced_accuracy = (specificity + sensitivity) / 2\n",
    "\n",
    "    print(f\"f1 score:{f1_score}\")\n",
    "    print(f\"accuracy:{accuracy}\")\n",
    "    print(f\"balanced_accuracy:{balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OS = True\n",
    "US = False\n",
    "WC = False\n",
    "\n",
    "X_train = np.load(\"Xtrain1.npy\")  \n",
    "Y_train = np.load(\"Ytrain1.npy\")  \n",
    "X_test = np.load(\"Xtest1.npy\")\n",
    "\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_normalized, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "if US:\n",
    "    print(\"undersampling\")\n",
    "    X_train_us, y_train_us = undersampling(X_train, y_train)\n",
    "    y_train_us = keras.utils.to_categorical(np.argmax(y_train_us, axis=1), num_classes=num_classes)\n",
    "\n",
    "elif OS:\n",
    "    print(\"oversampling\")\n",
    "    X_train_os, y_train_os = oversample_with_smote(X_train, y_train,Y_train)\n",
    "    y_train_os = keras.utils.to_categorical(np.argmax(y_train_os, axis=1), num_classes=num_classes)\n",
    "\n",
    "elif WC:\n",
    "    print(\"weighted classes\")\n",
    "    class_weight_dict = weigthed_classes(y_train)\n",
    "    print(class_weight_dict)\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "else:\n",
    "    print(\"imbalanced\")\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Int('filters1', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu',\n",
    "            input_shape=(48, 48, 1)\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.5, step=0.1)))  # Dropout layer\n",
    "\n",
    "        # Second convolutional layer\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Int('filters2', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Dropout(hp.Float('dropout2', min_value=0.2, max_value=0.5, step=0.1)))  \n",
    "\n",
    "        # Third convolutional layer\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Int('filters3', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Dropout(hp.Float('dropout3', min_value=0.2, max_value=0.5, step=0.1)))  \n",
    "\n",
    "        # Flatten and add Dense layers\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(units=hp.Int('units', min_value=64, max_value=256, step=64), activation='relu'))\n",
    "        model.add(layers.Dropout(0.5)) \n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Initialize the HyperModel\n",
    "hypermodel = CNNHyperModel()\n",
    "\n",
    "# Define the tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='cnn_tuning'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                            patience=5,\n",
    "                            restore_best_weights=True)\n",
    "\n",
    "X_train_reshaped = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val_reshaped = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "if OS:\n",
    "    print(\"OVERSAMPLING\")\n",
    "    X_train_os_reshaped = X_train_os.reshape(-1, 48, 48, 1)\n",
    "    tuner.search(X_train_os_reshaped, y_train_os, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(X_train_os_reshaped, y_train_os, epochs=100, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model.save('cnn_model_os.keras')\n",
    "    val_loss, val_accuracy = best_model.evaluate(X_val_reshaped, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "elif US:\n",
    "    print(\"UNDERSAMPLING\")\n",
    "    X_train_us_reshaped = X_train_us.reshape(-1, 48, 48, 1)\n",
    "    tuner.search(X_train_us_reshaped, y_train_us, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(X_train_us_reshaped, y_train_us, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model.save('cnn_model_us.keras')\n",
    "    val_loss, val_accuracy = best_model.evaluate(X_val_reshaped, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "elif WC:\n",
    "    print(\"WEIGHTED CLASSES\")\n",
    "    X_train_normalized_reshaped = X_train_normalized.reshape(-1, 48, 48, 1)\n",
    "    tuner.search(X_train_reshaped, y_train, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(X_train_reshaped, y_train, epochs=20,class_weight=class_weight_dict ,validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model.save('cnn_model_wc.keras')\n",
    "    val_loss, val_accuracy = best_model.evaluate(X_val_reshaped, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "else:\n",
    "    print(\"IMBALANCED\")\n",
    "    tuner.search(X_train_reshaped, y_train, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_model.fit(X_train_reshaped, y_train, epochs=20, validation_data=(X_val_reshaped, y_val), callbacks=[early_stopping])\n",
    "    best_model.save('cnn_model_imbalanced.keras')\n",
    "    val_loss, val_accuracy = best_model.evaluate(X_val_reshaped, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "#                                CNN - Model prediction\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"CNN\")\n",
    "\n",
    "if OS:\n",
    "    print(\"oversampling\")\n",
    "    model = load_model('best_cnn_model.keras')\n",
    "\n",
    "elif US:\n",
    "    print(\"undersampling\")\n",
    "    model = load_model('cnn_model_us.keras')\n",
    "\n",
    "elif WC:\n",
    "    print(\"weighted classes\")\n",
    "    model = load_model('cnn_model_wc.keras')\n",
    "\n",
    "else:\n",
    "    print(\"imbalanced\")\n",
    "    model = load_model('cnn_model_imbalanced.keras')\n",
    "\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "X_test_normalized = X_test_normalized.reshape(-1, 48, 48, 1)\n",
    "\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "np.save('Ytest1.npy',y_pred)\n",
    "\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "binary_predictions = model.predict(X_val)\n",
    "\n",
    "binary_predictions = np.argmax(binary_predictions, axis=1)\n",
    "\n",
    "y_val_reshaped = np.argmax(y_val, axis=1)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "#                                   CONFUSION MATRIX\n",
    "#---------------------------------------------------------------------------------------------\n",
    "print(classification_report(binary_predictions, y_val_reshaped, digits = 6))\n",
    "cm = confusion_matrix(binary_predictions, y_val_reshaped)\n",
    "\n",
    "if cm is not None:\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=np.unique(Y_train), \n",
    "                yticklabels=np.unique(Y_train))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)   \n",
    "    evaluate_model(cm)\n",
    "else:\n",
    "    print(\"A matriz de confusรฃo nรฃo foi criada. Verifique as condiรงรตes.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_npy(file_path, index, image_shape):\n",
    "\n",
    "    data = np.load(\"Xtest1.npy\")\n",
    "\n",
    "    image_vector = data[20]\n",
    "\n",
    "    image = image_vector.reshape(image_shape)\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    \n",
    "    plt.imshow(image, cmap='gray') \n",
    "    plt.title(f\"Image at Index {index}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "file_path = 'Xtrain1.npy' \n",
    "index = 16 \n",
    "image_shape = (48, 48, 1) \n",
    "\n",
    "display_image_from_npy(file_path, index, image_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
